---
id: 009
title: Clarify RAG Selected Text Usage
stage: spec
date: 2025-12-18
surface: agent
model: gemini-1.5-flash
feature: rag-qdrant
branch: 005-rag-qdrant
user: 
command: /sp.specify "Create a detailed technical specification for 'Phase 5: Use Qdrant Cloud and make a tool which OpenAI Agent uses for RAG purpose. Also sync your book content into vectordb.' Project Context: This is the final development phase (Phase 5) of a hackathon to build an AI-native 'Physical AI & Humanoid Robotics' textbook. Phase 1: Book content (Markdown files in Docusaurus). Phase 2: FastAPI backend with a generic /chat endpoint. Phase 3/4: Themed Chatbot UI. This Phase 5: Implement Retrieval-Augmented Generation (RAG) so the chatbot answers are grounded in the textbook content. This involves creating a vector database (Qdrant Cloud), syncing the book into it, and modifying the backend to perform retrieval before generating an answer. Core Objective: Enhance the Phase 2 backend to become a RAG pipeline. When a user asks a question, the system will first search the textbook's vector database for relevant content and then instruct the OpenAI model to answer based only on that retrieved context. This ensures accurate, book-specific answers. Detailed Requirements: 1. RAG System Architecture: Data Flow: User Query → Backend → Query Embedding → Qdrant Vector Search → Retrieve Relevant Text Passages → Construct Context-Aware Prompt → Call OpenAI API → Return Grounded Answer. Components: 1. Embedding Model: Use text-embedding-3-small or similar via OpenAI API. 2. Vector Database: Qdrant Cloud (serverless) to store and search document embeddings. 3. RAG Orchestrator: New backend service/module that coordinates the retrieval and generation steps. 2. Content Processing & Sync Pipeline ("Tool"): Source: All book content Markdown files from Phase 1 (Docusaurus docs/ directory). Processing Steps: 1. Chunking: Split Markdown content into semantically meaningful chunks (e.g., by section, ~200-400 words). 2. Embedding: Generate a vector embedding for each text chunk using the chosen embedding model. 3. Storage: Store each chunk along with its embedding, original text, and metadata (source file, chapter, heading) as a point in a Qdrant collection. Sync Tool: Create a script (sync_book_to_qdrant.py) that can be run to (re)process the book content and update the vector database. This is crucial for the hackathon deliverable. 3. Backend (Phase 2) Modifications: New RAG Service: Create a rag_service.py module with a function get_relevant_context(query: str, limit: int = 3) that handles the Qdrant search. Enhanced /chat Endpoint Logic: Modify the endpoint to: 1. Accept the user query. 2. (Optional) Use selected_text from the request as a high-priority filter or addition to the query. 3. Call get_relevant_context(query) to retrieve the top N most relevant book passages. 4. Construct a new, stricter system prompt: "You are an assistant for the 'Physical AI & Humanoid Robotics' textbook. Answer the user's question based ONLY on the following context from the book. If the answer isn't in the context, say so. Context: {retrieved_passages}". 5. Proceed with the OpenAI call and response as before. New Endpoint (Optional): POST /ingest (admin) to trigger the book sync process. 4. Qdrant Cloud Setup & Data Model: Collection Name: physical_ai_textbook Vector Configuration: Size matching the chosen embedding model (e.g., 1536 for text-embedding-3-small). Payload (Metadata) Schema: { "text": str, "source": str, "chapter": str, "section": str } 5. Integration with Hackathon Bonus Features: The selected_text feature should seamlessly integrate. If provided, the RAG search should heavily favor or exclusively search within chunks originating from the same section as the selected text. The system's ability to cite sources (mention the chapter/section) should be considered a valuable enhancement. 6. Acceptance Criteria (Checklist): [ ] Qdrant Cloud collection is created and accessible. [ ] Sync tool successfully processes all Phase 1 book Markdown files and populates the vector DB. [ ] The modified /chat endpoint performs a vector search for each query. [ ] Chatbot answers are demonstrably grounded in the provided book context. [ ] The system correctly handles cases where the book context doesn't contain an answer. [ ] The selected_text bonus feature integrates with the RAG flow. Output: Generate a complete spec.md file with these sections: 1. Overview & RAG Architecture Diagram 2. Data Pipeline Specification (Chunking, Embedding, Sync Tool) 3. Qdrant Cloud Configuration & Schema 4. Backend Modification Requirements (Service, Endpoint Logic, Prompts) 5. Integration Plan with Existing Phases (2, 3) 6. Validation & Testing Strategy 7. Acceptance Criteria"
response_text: |
  I have generated the initial `spec.md` and `checklists/requirements.md` for the `005-rag-qdrant` feature.

  Awaiting clarification on how `selected_text` should be used to bias the Qdrant search. Please provide your choice from `clarification_q1.md`.
---